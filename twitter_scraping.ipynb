{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hybrid html 'selenium' parse with 'beautiful soup' lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from os.path import exists\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time\n",
    "import random as rnd\n",
    "# import types\n",
    "import twitter_config as cfg\n",
    "\n",
    "DATA_ROOT = 'twitter_data/'\n",
    "\n",
    "class Webbroswer(object):\n",
    "  def __init__(self, **params):\n",
    "    self.user = params.get('username', None)\n",
    "    self.passwd = params.get('password', None)\n",
    "    self.verify = params.get('verify', None)\n",
    "    self.driver = params.get('driver', 'Chrome')\n",
    "    self.mobile = params.get('mobile', False)\n",
    "    self.proxy = params.get('proxy', False)\n",
    "    assert self.user is not None, '\"username\" param required'\n",
    "    assert self.passwd is not None, '\"password\" param required'\n",
    "    assert self.verify is not None, '\"verify\" param required'\n",
    "    assert self.driver in ['Chrome', 'Firefox', 'Safari', 'IE'], 'unsupported browser type'\n",
    "    self.cookies_file = ''.join([DATA_ROOT, self.verify, '_', self.driver, '.cookies'])\n",
    "    self.browsing_file = self.cookies_file.replace('.cookies', '.json')\n",
    "    self.cookies_cache = []\n",
    "    self.browser = None\n",
    "    self.url = None\n",
    "\n",
    "  def clear_browsing(self):\n",
    "    self.url = cfg.BASE_URL\n",
    "\n",
    "  def save_browsing(self):\n",
    "    with open(self.browsing_file, \"w\") as file:\n",
    "      file.write(self.url)\n",
    "\n",
    "  def load_browsing(self):\n",
    "    if not exists(self.browsing_file): return\n",
    "    with open(self.browsing_file, \"r\") as file:\n",
    "      self.url = file.read()\n",
    "\n",
    "  def save_cookies(self):\n",
    "    self.cookies_cache = self.browser.get_cookies()\n",
    "    with open(self.cookies_file, \"wb\") as file:\n",
    "      pickle.dump(self.cookies_cache, file)\n",
    "\n",
    "  def load_cookies(self):\n",
    "    if not exists(self.cookies_file): return\n",
    "    with open(self.cookies_file, \"rb\") as file:\n",
    "      self.cookies_cache = pickle.load(file)\n",
    "\n",
    "  def apply_cookies(self):\n",
    "    for cookie in self.cookies_cache:\n",
    "      self.browser.add_cookie(cookie)\n",
    "\n",
    "  def open_browser(self, url=None):\n",
    "    # --- browser driver obj\n",
    "    if self.driver == 'Chrome':\n",
    "      browser_options = webdriver.ChromeOptions()\n",
    "      if self.mobile:\n",
    "        browser_options.add_experimental_option('mobileEmulation', {'deviceName':'Nexus 7'})\n",
    "      if self.proxy:\n",
    "        browser_options.add_argument(f'--proxy-server={self.proxy}')\n",
    "      self.browser = webdriver.Chrome(options=browser_options)\n",
    "    elif self.driver == 'Firefox':\n",
    "      self.browser = webdriver.Firefox()\n",
    "    elif self.driver == 'Safari':\n",
    "      self.browser = webdriver.Safari()\n",
    "    elif self.driver == 'IE':\n",
    "      self.browser = webdriver.Ie()\n",
    "    self.browser.implicitly_wait(10)\n",
    "    # --- load data\n",
    "    self.load_cookies()\n",
    "    # --- set url\n",
    "    if url is None: self.load_browsing()\n",
    "    else: self.url = url\n",
    "    if self.url is None: self.url = cfg.BASE_URL\n",
    "    # --- open url, apply cookies\n",
    "    self.browser.get(self.url)\n",
    "    self.apply_cookies()\n",
    "    self.browser.refresh()\n",
    "\n",
    "  def close_browser(self, clear_browsing=False):\n",
    "    if self.browser:\n",
    "      self.save_cookies()\n",
    "      if clear_browsing: self.clear_browsing()\n",
    "      self.save_browsing()\n",
    "      self.browser.quit()\n",
    "      self.url = None\n",
    "\n",
    "  def open_page(self, url=cfg.BASE_URL, forceRefresh=False):\n",
    "    refresh_needed = forceRefresh\n",
    "    if self.browser is None:\n",
    "      self.open_browser(url='about:blank')\n",
    "      refresh_needed = True\n",
    "    if url != self.url:\n",
    "      self.browser.get(url)\n",
    "      self.url = url\n",
    "    if refresh_needed:\n",
    "      self.browser.refresh()\n",
    "\n",
    "  def logged_in(self):\n",
    "    self._random_timeout()\n",
    "    logged_in = False\n",
    "    try:\n",
    "      verify_field = self.browser.find_element(By.XPATH, \"//*[@data-testid='login']\")\n",
    "    except NoSuchElementException:\n",
    "      logged_in = True\n",
    "    finally:\n",
    "      return logged_in\n",
    "\n",
    "  def login_username(self, username):\n",
    "    login_required = False\n",
    "    try:\n",
    "      username_field = self.browser.find_element(By.XPATH, \"//*[@autocomplete='username']\")\n",
    "      username_field.send_keys(username)\n",
    "      next_button = self.browser.find_element(By.XPATH, \"//*[text()='Next']/../..\")\n",
    "      next_button.click()\n",
    "      login_required = True\n",
    "    except NoSuchElementException:\n",
    "      print('seems already logged in')\n",
    "    finally:\n",
    "      return login_required\n",
    "\n",
    "  def login_verify(self, verify):\n",
    "    self._random_timeout()\n",
    "    try:\n",
    "      verify_field = self.browser.find_element(By.XPATH, \"//*[@data-testid='ocfEnterTextTextInput']\")\n",
    "      verify_field.send_keys(verify)\n",
    "      next_button = self.browser.find_element(By.XPATH, \"//*[text()='Next']/../..\")\n",
    "      next_button.click()\n",
    "    except NoSuchElementException:\n",
    "      print('verify name skipped')\n",
    "\n",
    "  def login_password(self, password):\n",
    "    try:\n",
    "      password_field = self.browser.find_element(By.XPATH, \"//*[@autocomplete='current-password']\")\n",
    "      password_field.send_keys(password)\n",
    "      login_button = self.browser.find_element(By.XPATH, \"//*[text()='Log in']/../..\")\n",
    "      login_button.click()\n",
    "    except NoSuchElementException:\n",
    "      print('password skipped')\n",
    "\n",
    "  def login(self):\n",
    "    self.browser.implicitly_wait(10)\n",
    "    self.open_page(cfg.LOGIN_URL)\n",
    "    self._random_sleep(2)\n",
    "    if self.login_username(self.user):\n",
    "      self._random_sleep(2)\n",
    "      self.login_verify(self.verify)\n",
    "      self._random_sleep(2)\n",
    "      self.login_password(self.passwd)\n",
    "\n",
    "  def click_following_on_homepage(self):\n",
    "    if self.browser.current_url != cfg.HOME_URL:\n",
    "      self.open_page(cfg.HOME_UR, True)\n",
    "    self.browser.implicitly_wait(0.3)\n",
    "    try:\n",
    "      following_tab = self.browser.find_element(By.XPATH, \"//*[@data-testid='ScrollSnap-List']//span[text()='Following']\")\n",
    "      # next_button = self.browser.find_element(By.XPATH, \"//*[text()='Next']/../..\")\n",
    "      following_tab.click()\n",
    "    except NoSuchElementException:\n",
    "      print('following tab not found')\n",
    "\n",
    "  def scroll_to_bottom(self, scroll_round=1):\n",
    "    SCROLL_PAUSE_TIME = 2\n",
    "    # scroll to bottom until no more content loaded\n",
    "    last_height = self.browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    for i in range(scroll_round):\n",
    "      self.browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "      time.sleep(SCROLL_PAUSE_TIME)\n",
    "      new_height = self.browser.execute_script(\"return document.body.scrollHeight\")\n",
    "      if new_height == last_height:\n",
    "        break\n",
    "      last_height = new_height\n",
    "\n",
    "  def _random_timeout(self, least=0.5):\n",
    "    self.browser.implicitly_wait(rnd.random() + least)\n",
    "\n",
    "  def _random_sleep(self, least=1, dimension=1):\n",
    "    time.sleep(rnd.random()*dimension + least)\n",
    "\n",
    "  def _pick_tweet_text(self, func):\n",
    "    text_element = func(attrs={\"data-testid\":\"tweetText\"})\n",
    "    return text_element.text if text_element else None\n",
    "\n",
    "  def _pick_tweet_media_items(self, func, attrs, sel):\n",
    "    divs = func(attrs=attrs)\n",
    "    elements = []\n",
    "    for div in divs:\n",
    "      _elments = [e['src'] for e in div.select(sel)]\n",
    "      elements = chain(elements, _elments)\n",
    "    # return list(elements) # element duplicated\n",
    "    return list(dict.fromkeys(elements))\n",
    "\n",
    "  def _pick_tweet_imgs(self, func):\n",
    "    return self._pick_tweet_media_items(func, {\"data-testid\":\"tweetPhoto\"}, 'img[src]')\n",
    "\n",
    "  def _pick_tweet_videos(self, func):\n",
    "    return self._pick_tweet_media_items(func, {\"data-testid\":\"videoComponent\"}, 'video[src]')\n",
    "\n",
    "  def _pick_tweet_stats(self, func):\n",
    "    elements = func(attrs={\"data-testid\":\"app-text-transition-container\"})\n",
    "    if len(elements) == 0: return None\n",
    "    tweet_stats = [x.text for x in elements] # [ reply, repost, like, view]\n",
    "    stats_len = len(tweet_stats)\n",
    "    return {\n",
    "      'reply':  tweet_stats[0] if stats_len > 0 else None,\n",
    "      'repost': tweet_stats[1] if stats_len > 1 else None,\n",
    "      'like':   tweet_stats[2] if stats_len > 2 else None,\n",
    "      'view':   tweet_stats[3] if stats_len > 3 else None\n",
    "    }\n",
    "\n",
    "  def _pick_tweek_link(self, func):\n",
    "    te = func('time')\n",
    "    if te is None: return None\n",
    "    element = te.find_parent('a')\n",
    "    return element['href'] if element else None\n",
    "\n",
    "  def _pick_tweet_timestamp(self, func):\n",
    "    element = func('time')\n",
    "    if element is None: return None\n",
    "    timestamp = element['datetime']\n",
    "    return datetime.fromisoformat(timestamp.rstrip('Z')).replace(tzinfo=timezone.utc)\n",
    "\n",
    "  def _pick_quote_owner(self, func):\n",
    "    user_element = func(attrs={\"data-testid\":\"User-Name\"})\n",
    "    ne = user_element.find(lambda tag: tag.string and tag.string.startswith('@'))\n",
    "    return ne.text[1:] if ne else None\n",
    "\n",
    "  def _pick_quote_link(self, func):\n",
    "    text_element = func(attrs={\"data-testid\":\"tweetText\"})\n",
    "    element = text_element.find('a')\n",
    "    return element['href'] if element else None\n",
    "\n",
    "  def _gen_res(self, res_map):\n",
    "    if not bool(res_map): return None\n",
    "    res = {}\n",
    "    for res_type in res_map:\n",
    "      f = res_map[res_type]['f']\n",
    "      arg = res_map[res_type]['arg']\n",
    "      res[res_type] = f(arg)\n",
    "    return res\n",
    "\n",
    "  def _parse_tweet(self, tweet):\n",
    "    tweet_anchor = tweet\n",
    "    tweet_find_name = 'find_next'\n",
    "    tweet_find_all_name = 'find_all_next'\n",
    "\n",
    "    quote_res_map = None\n",
    "    quote = tweet.find('span', string='Quote')\n",
    "    if quote:\n",
    "      tweet_anchor = quote\n",
    "      tweet_find_name = 'find_previous'\n",
    "      tweet_find_all_name = 'find_all_previous'\n",
    "\n",
    "      quote_find = getattr(quote, 'find_next')\n",
    "      quote_find_all = getattr(quote, 'find_all_next')\n",
    "      quote_res_map = {\n",
    "        'owner': {'f': self._pick_quote_owner, 'arg': quote_find},\n",
    "        'link': {'f': self._pick_quote_link, 'arg': quote_find},\n",
    "        'text': {'f': self._pick_tweet_text, 'arg': quote_find},\n",
    "        'timestamp': {'f': self._pick_tweet_timestamp, 'arg':quote_find},\n",
    "        'imgs': {'f': self._pick_tweet_imgs, 'arg': quote_find_all},\n",
    "        'videos': {'f': self._pick_tweet_videos, 'arg':quote_find_all},\n",
    "      }\n",
    "\n",
    "    tweet_find = getattr(tweet_anchor, tweet_find_name)\n",
    "    tweet_find_all = getattr(tweet_anchor, tweet_find_all_name)\n",
    "    tweet_res_map = {\n",
    "      'text': {'f': self._pick_tweet_text, 'arg': tweet_find},\n",
    "      'timestamp': {'f': self._pick_tweet_timestamp, 'arg':tweet_find},\n",
    "      'link': {'f': self._pick_tweek_link, 'arg': tweet_find},\n",
    "      'imgs': {'f': self._pick_tweet_imgs, 'arg': tweet_find_all},\n",
    "      'videos': {'f': self._pick_tweet_videos, 'arg':tweet_find_all},\n",
    "      'stats': {'f': self._pick_tweet_stats, 'arg': getattr(tweet, 'find_all')}\n",
    "    }\n",
    "\n",
    "    return {\n",
    "      'tweet': self._gen_res(tweet_res_map),\n",
    "      'quote': self._gen_res(quote_res_map)\n",
    "    }\n",
    "\n",
    "  def tweets_from_html(self, html):\n",
    "    # soup = BeautifulSoup(html, 'html.parser')\n",
    "    # soup = BeautifulSoup(html, \"html5lib\")\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    tweets = soup.find_all(attrs={\"data-testid\":\"tweet\"})\n",
    "    ret = [self._parse_tweet(t.extract()) for t in tweets]\n",
    "    return ret\n",
    "\n",
    "  def html_of_browser_element(self, element):\n",
    "    return self.browser.execute_script(\"return arguments[0].outerHTML;\", element) \n",
    "\n",
    "  def first_load_html(self):\n",
    "    # --- select the root element and use BS4 to parse it\n",
    "    # tweets_text = self.browser.find_elements(By.XPATH, \"//*[@data-testid='tweetText']/span\")\n",
    "    # tweets_text = browser.find_elements(By.CSS_SELECTOR, \"[data-testid='tweetText']\")\n",
    "    # tweets = self.browser.find_elements(By.XPATH, \"//*[@data-testid='tweet']\")\n",
    "    html_segment = self.browser.find_element(By.XPATH, \"//*[@data-testid='primaryColumn']\")\n",
    "    return self.html_of_browser_element(html_segment)\n",
    "\n",
    "  def _find(self, node, method, pattern, timeout=2):\n",
    "    self._random_timeout(0.1)\n",
    "    find_func = getattr(node, method)\n",
    "    elapsed_time = 0\n",
    "    ret = None\n",
    "    while ret is None and elapsed_time < timeout:\n",
    "      tn = time.time()\n",
    "      try:\n",
    "        ret = find_func(By.XPATH, pattern)\n",
    "      except NoSuchElementException:\n",
    "        ret = None\n",
    "      except StaleElementReferenceException:\n",
    "        ret = None\n",
    "        break\n",
    "      elapsed_time += time.time() - tn\n",
    "\n",
    "    return ret\n",
    "\n",
    "  def find_element(self, node, pattern, timeout=2):\n",
    "    return self._find(node=node, method='find_element', pattern=pattern, timeout=timeout)\n",
    "\n",
    "  def find_elements(self, node, pattern, timeout=2):\n",
    "    return self._find(node=node, method='find_elements', pattern=pattern, timeout=timeout)\n",
    "\n",
    "  def merge_tweets(self, tweets, more_tweets):\n",
    "    for tweet in more_tweets:\n",
    "      if all(t['tweet']['link'] != tweet['tweet']['link'] for t in tweets):\n",
    "        tweets.append(tweet)\n",
    "    return tweets\n",
    "\n",
    "  def load_more_tweets(self, node, pgdn_count=1, timeout=2):\n",
    "    # press PgDn key to load more tweets\n",
    "    for ct in range(pgdn_count):\n",
    "      self.browser.switch_to.active_element.send_keys(Keys.PAGE_DOWN)\n",
    "      self._random_sleep(0.1, 0.9)\n",
    "\n",
    "    # load more divs; parse more tweet divs as tweets (list of tweet dict)\n",
    "    more_tweet_divs = self.find_elements(node=node,\n",
    "                                         pattern=\"following-sibling::*[@data-testid='cellInnerDiv']\",\n",
    "                                         timeout=timeout)\n",
    "    more_tweets = []\n",
    "    if more_tweet_divs is not None:\n",
    "      for tweet_div in more_tweet_divs:\n",
    "        div_tweets = self.tweets_from_html(self.html_of_browser_element(tweet_div))\n",
    "        more_tweets = chain(more_tweets, div_tweets)\n",
    "      more_tweets = list(more_tweets)\n",
    "\n",
    "    return more_tweets, more_tweet_divs\n",
    "  \n",
    "  def _parse_tracetime(self, traceback):\n",
    "    time_unit_map = { 's': 'seconds', 'm': 'minutes', 'h': 'hours', 'd': 'days', 'w': 'weeks' }\n",
    "    value = int(traceback[:-1])\n",
    "    unit = traceback[-1]\n",
    "    if unit not in time_unit_map:\n",
    "      raise ValueError(f\"Unknown time unit: {unit}\")\n",
    "    return timedelta(**{time_unit_map[unit]: value})\n",
    "\n",
    "  def _tweets_after_time(self, tweets, targettime, timestamps={}, stat={'miss':0,'meet':0, 'stable':0}):\n",
    "    miss = 0\n",
    "    for tweet in tweets:\n",
    "      link = tweet['tweet']['link']\n",
    "      if link is None: continue # skip non tweet\n",
    "      split = link.split('/')\n",
    "      account = split[1]\n",
    "      timestamp = tweet['tweet']['timestamp']\n",
    "      timestamps[account] = timestamp\n",
    "      if timestamp < targettime: stat['meet'] += 1\n",
    "      else: miss += 1\n",
    "    # stat['weight'] = stat['meet'] / (miss + stat['miss'])\n",
    "    stat['miss'] += miss\n",
    "    if miss > 0: stat['stable'] = 0\n",
    "    else: stat['stable'] += 1\n",
    "    # stat['weight'] = stat['miss'] / (miss + stat['miss'])\n",
    "    return stat, timestamps\n",
    "\n",
    "  def load_tweets(self, traceback=None, stable=5, more_round=0, timeout=10):\n",
    "    # on page loaded, find tweet cell divs\n",
    "    tweet_divs = self.find_elements(node=self.browser, pattern=\"//*[@data-testid='cellInnerDiv']\", timeout=timeout)\n",
    "    if not tweet_divs: return []\n",
    "\n",
    "    # parse existing tweets from html of browser\n",
    "    tweets = self.tweets_from_html(self.first_load_html())\n",
    "\n",
    "    # traceback target time\n",
    "    if traceback:\n",
    "      # check tweets' time back to time of traceback point\n",
    "      targettime = datetime.now(timezone.utc) - self._parse_tracetime(traceback)\n",
    "      stat, timestamps = self._tweets_after_time(tweets, targettime)\n",
    "\n",
    "      # load more if most of tweets' time not yet reach back to trace point\n",
    "      last_div = tweet_divs[-1]\n",
    "      while stat['stable'] < stable:\n",
    "        print(f'stable: {stat['stable']:.2f}, meet: {stat['meet']}, miss: {stat['miss']}')\n",
    "        more_tweets, more_divs = self.load_more_tweets(node=last_div, pgdn_count=1, timeout=timeout)\n",
    "        tweets = self.merge_tweets(tweets, more_tweets)\n",
    "        if more_divs:\n",
    "          stat, timestamps = self._tweets_after_time(more_tweets, targettime, timestamps, stat)\n",
    "          last_div = more_divs[-1]\n",
    "        else:\n",
    "          time.sleep(0.1)\n",
    "        # break\n",
    "    # load more if 'more_round' given\n",
    "    else:\n",
    "      for round in range(more_round):\n",
    "        more_tweets, more_divs = self.load_more_tweets(node=last_div, pgdn_count=1, timeout=timeout)\n",
    "        tweets = self.merge_tweets(tweets, more_tweets)\n",
    "        if more_divs:\n",
    "          last_div = more_divs[-1]\n",
    "        else:\n",
    "          time.sleep(0.1)\n",
    "          # break\n",
    "\n",
    "    return tweets\n",
    "\n",
    "  def load_url_tweets(self, url, traceback=None, stable=5, more_round=0, timeout=10):\n",
    "    self.open_page(url, True)\n",
    "    return self.load_tweets(traceback, stable, more_round, timeout)\n",
    "\n",
    "  def wait_page_loading(self, timeout=30):\n",
    "    tweet_div = self.find_element(node=self.browser, pattern=\"//*[@data-testid='cellInnerDiv']\", timeout=timeout)\n",
    "    return True if tweet_div else False\n",
    "\n",
    "  def follow_accounts(self, accounts):\n",
    "    for account in accounts:\n",
    "      url = cfg.BASE_URL + '/' + account\n",
    "      self.open_page(url)\n",
    "      follow_button = self.find_element(self.browser, \"//*[@data-testid='placementTracking']//span[text()='Follow']\")\n",
    "      if follow_button: follow_button.click()\n",
    "      time.sleep(0.2)\n",
    "\n",
    "  def unfollow_accounts(self, accounts):\n",
    "    for account in accounts:\n",
    "      url = cfg.BASE_URL + '/' + account\n",
    "      self.open_page(url)\n",
    "      # find 'Following' button to popup; then click 'Unfollow'\n",
    "      popup = self.find_element(self.browser, \"//*[@data-testid='placementTracking']//span[text()='Following']\")\n",
    "      if popup:\n",
    "        popup.click()\n",
    "        time.sleep(0.2)\n",
    "        unfollow = self.find_element(self.browser, \"//*[@data-testid='confirmationSheetConfirm']//span[text()='Unfollow']\")\n",
    "        if unfollow: unfollow.click()\n",
    "      else:\n",
    "        # try to find unfollow icon to click\n",
    "        icon = self.find_element(self.browser, f\"//*[@aria-label='Unfollow @{account}']\")\n",
    "        if icon:\n",
    "          icon.click()\n",
    "          unfollow = self.find_element(self.browser, f\"//*[@data-testid='Dropdown']//span[text()='Unfollow @{account}']\")\n",
    "          if unfollow: unfollow.click()\n",
    "      time.sleep(0.2)\n",
    "\n",
    "  def print(self, tweets):\n",
    "    print(f' num of tweets: {len(tweets)} '.center(50, '-'))\n",
    "    LB_PAD = 6\n",
    "    def print_l(label, lines, label_pad=LB_PAD):\n",
    "      if isinstance(lines, list):\n",
    "        if len(lines) > 0:\n",
    "          print(label.ljust(label_pad, ' '), lines[0])\n",
    "          for l in lines[1:]: print(' '*LB_PAD, l)\n",
    "        else:\n",
    "          print(label.ljust(label_pad, ' '), None)\n",
    "      else:\n",
    "        print(label.ljust(label_pad, ' '), lines)\n",
    "  \n",
    "    for i in range(0, len(tweets)):\n",
    "      t = tweets[i]\n",
    "      print(f' tweet<{i}> '.center(50, '-'))\n",
    "      print_l('link:', t['tweet']['link'])\n",
    "      print_l('text:', t['tweet']['text'].split('\\n') if  t['tweet']['text'] else None)\n",
    "      print_l('imgs:', t['tweet']['imgs'])\n",
    "      print_l('vids:', t['tweet']['videos'])\n",
    "      print_l('stats:',t['tweet']['stats'])\n",
    "      print_l('date:', t['tweet']['timestamp'].astimezone() if t['tweet']['timestamp'] else None)\n",
    "      if t['quote']:\n",
    "        print(' quote '.center(30, '~'))\n",
    "        print_l('owner:',t['quote']['owner'])\n",
    "        print_l('text:',  t['quote']['text'].split('\\n') if t['quote']['text'] else None)\n",
    "        print_l('link:',  t['quote']['link'] if t['quote']['link'] else None)\n",
    "        print_l('imgs:',  t['quote']['imgs'])\n",
    "        print_l('vids:',  t['quote']['videos'])\n",
    "        print_l('date:',  t['quote']['timestamp'].astimezone() if t['quote']['timestamp'] else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================== https://twitter.com/home ===========================\n",
      "stable: 0.00, meet: 3, miss: 7\n",
      "stable: 1.00, meet: 9, miss: 7\n",
      "stable: 2.00, meet: 12, miss: 7\n",
      "stable: 3.00, meet: 15, miss: 7\n",
      "stable: 4.00, meet: 17, miss: 7\n",
      "------ elapsed: 5.33s\n",
      "--------------- num of tweets: 25 ----------------\n",
      "-------------------- tweet<0> --------------------\n",
      "link:  /Law360/status/1734203905616347381\n",
      "text:  Welcome to Monday and a quick roundup of some of Law360's biggest news.\n",
      "imgs:  https://pbs.twimg.com/media/GBEhQvsbMAAGHjZ?format=jpg&name=small\n",
      "vids:  None\n",
      "stats: {'reply': '1', 'repost': '', 'like': '', 'view': '245'}\n",
      "date:  2023-12-11 21:30:05+08:00\n",
      "-------------------- tweet<1> --------------------\n",
      "link:  /Law360/status/1734205167724319054\n",
      "text:  The D.C. Circuit issued a narrowed gag order restraining Donald Trump's public statements amid his criminal election-interference case after finding a lower court restricted \"more protected speech than is necessary.\"\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '', 'repost': '', 'like': '', 'view': '58'}\n",
      "date:  2023-12-11 21:35:06+08:00\n",
      "-------------------- tweet<2> --------------------\n",
      "link:  /TheBlock__/status/1734205007585533976\n",
      "text:  Animoca Brands, Chainalysis among crypto firms targeted by $500 million fund: report\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '5', 'repost': '1', 'like': '5', 'view': '1K'}\n",
      "date:  2023-12-11 21:34:28+08:00\n",
      "-------------------- tweet<3> --------------------\n",
      "link:  /CoinDesk/status/1734204513127460930\n",
      "text:  Bitcoin dropped after surging last week to yearly highs and Goldman Sachs brought forward its estimate for the Fed’s first interest-rate cut. @LedesmaLyllah and @godbole17 report in First Mover Americas.\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '10', 'repost': '3', 'like': '15', 'view': '3.3K'}\n",
      "date:  2023-12-11 21:32:30+08:00\n",
      "-------------------- tweet<4> --------------------\n",
      "link:  /BTCTN/status/1734204288014950574\n",
      "text:  #Stablecoin Supply Turns Positive for the First Time in Nearly Two Years — Study #zeroknowledge #defi\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '21', 'repost': '2', 'like': '15', 'view': '1.9K'}\n",
      "date:  2023-12-11 21:31:36+08:00\n",
      "-------------------- tweet<5> --------------------\n",
      "link:  /Cointelegraph/status/1734198993255817479\n",
      "text:  French startup Mistral AI closes $415M funding round\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '7', 'repost': '7', 'like': '21', 'view': '6.3K'}\n",
      "date:  2023-12-11 21:10:34+08:00\n",
      "-------------------- tweet<6> --------------------\n",
      "link:  /BTCTN/status/1734196337426551122\n",
      "text:  .@Snowden Blasts #ElizabethWarren as 'Pro-Banker,' Claims She 'Rolls Over' for JPMorgan Boss in #Crypto Clash  #cryptonews\n",
      "imgs:  https://pbs.twimg.com/ext_tw_video_thumb/1734112343083438080/pu/img/TdCwCEiHgDZZT5Ni.jpg\n",
      "vids:  blob:https://twitter.com/ef75d7a3-4c87-4d58-8b6b-148aa50ddc39\n",
      "stats: {'reply': '36', 'repost': '11', 'like': '40', 'view': '6.2K'}\n",
      "date:  2023-12-11 21:00:01+08:00\n",
      "-------------------- tweet<7> --------------------\n",
      "link:  /BTCTN/status/1734189482293723287\n",
      "text:  Google Says It Has Updated Policy Relating #Crypto Coin Trusts Adverts #cryptocurrency #cryptotrust\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '31', 'repost': '23', 'like': '66', 'view': '9.6K'}\n",
      "date:  2023-12-11 20:32:46+08:00\n",
      "-------------------- tweet<8> --------------------\n",
      "link:  /decryptmedia/status/1734188900724134356\n",
      "text:  Bitcoin's upward march over the past month hit a bump in the road Monday morning, with the cryptocurrency's price slipping below $43,000.\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '', 'repost': '2', 'like': '7', 'view': '1.6K'}\n",
      "date:  2023-12-11 20:30:28+08:00\n",
      "-------------------- tweet<9> --------------------\n",
      "link:  /Cointelegraph/status/1734089207742648623\n",
      "text:  Crypto exchange @HTX_Global has witnessed some $258 million in net outflows in the weeks following a $30 million exploit on Nov. 22.\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '15', 'repost': '22', 'like': '69', 'view': '22K'}\n",
      "date:  2023-12-11 13:54:19+08:00\n",
      "------------------- tweet<10> --------------------\n",
      "link:  /Cointelegraph/status/1734185008376741946\n",
      "text:  Over the past two months, HTX and other Sun-linked entities, such as  crypto exchange Poloniex and the HTX Eco Chain (HECO) bridge, have been hacked a total of four times.\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '2', 'repost': '4', 'like': '13', 'view': '7.3K'}\n",
      "date:  2023-12-11 20:15:00+08:00\n",
      "------------------- tweet<11> --------------------\n",
      "link:  /CoinDesk/status/1734179896677605725\n",
      "text:  .@Barry Silbert's @DCGco overcame a turbulent year with strategic decisions and settlements, reaffirming its place in the crypto world. \n",
      "       \n",
      "       That’s why he’s among CoinDesk’s #MostInfluential2023: https://trib.al/yWcJXQg\n",
      "imgs:  https://pbs.twimg.com/media/GBELbRHWgAAGfs8?format=jpg&name=small\n",
      "vids:  None\n",
      "stats: {'reply': '14', 'repost': '5', 'like': '25', 'view': '14K'}\n",
      "date:  2023-12-11 19:54:41+08:00\n",
      "------------------- tweet<12> --------------------\n",
      "link:  /TheBlock__/status/1734178634506609116\n",
      "text:  Crypto fund inflows hit $43 million, marking 11-week streak totaling $1.8 billion\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '3', 'repost': '8', 'like': '21', 'view': '6.2K'}\n",
      "date:  2023-12-11 19:49:40+08:00\n",
      "------------------- tweet<13> --------------------\n",
      "link:  /BTCTN/status/1734174669123494090\n",
      "text:  Financial services giant #Fidelity recently met with the #SEC regarding its spot #bitcoin exchange-traded fund (ETF) application.\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '23', 'repost': '43', 'like': '166', 'view': '15K'}\n",
      "date:  2023-12-11 19:33:55+08:00\n",
      "------------------- tweet<14> --------------------\n",
      "link:  /CoinDesk/status/1734174337685430331\n",
      "text:  .@aeyakovenko's vision for @Solana as a robust DeFi ecosystem bore fruit in 2023 with a 500% rise in $SOL. \n",
      "       \n",
      "       He had a year of tech innovation and resilience. That's why he's part of CoinDesk's #MostInfluential2023:\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '12', 'repost': '3', 'like': '45', 'view': '17K'}\n",
      "date:  2023-12-11 19:32:36+08:00\n",
      "------------------- tweet<15> --------------------\n",
      "link:  /Cointelegraph/status/1734085508064485716\n",
      "text:  Google’s crypto-related ad policy has been updated to allow advertisements by “Cryptocurrency Coin Trusts” targeting the United States.\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '24', 'repost': '30', 'like': '92', 'view': '23K'}\n",
      "date:  2023-12-11 13:39:37+08:00\n",
      "------------------- tweet<16> --------------------\n",
      "link:  /Cointelegraph/status/1734173685634757098\n",
      "text:  Cryptocurrency Coin Trusts were exampled as “financial products that  allow investors to trade shares in trusts holding large pools of digital  currency” — likely including ETFs.\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '3', 'repost': '1', 'like': '17', 'view': '7.9K'}\n",
      "date:  2023-12-11 19:30:00+08:00\n",
      "------------------- tweet<17> --------------------\n",
      "link:  /lex_node/status/1734173131219087792\n",
      "text:  the aggregate supply of elite Ethereum researchers and elite Cosmos devs is permanently capped at 9,996 individuals\n",
      "imgs:  https://pbs.twimg.com/media/GBEFMnwW0AAcCIH?format=png&name=360x360\n",
      "       https://pbs.twimg.com/media/GBEFD7PWUAA7h74?format=png&name=360x360\n",
      "       https://pbs.twimg.com/media/GBEE2MpXwAAp4Lh?format=png&name=360x360\n",
      "vids:  None\n",
      "stats: {'reply': '2', 'repost': '2', 'like': '19', 'view': '3.4K'}\n",
      "date:  2023-12-11 19:27:48+08:00\n",
      "~~~~~~~~~~~ quote ~~~~~~~~~~~~\n",
      "owner: lilgainzz\n",
      "text:  Imagine having ZERO exposure to @StargazeZone NFTs. \n",
      "       \n",
      "       Gabriel knows what’s up. twitter.com/lex_node/statu…\n",
      "link:  None\n",
      "imgs:  None\n",
      "vids:  None\n",
      "date:  2023-12-11 18:07:15+08:00\n",
      "------------------- tweet<18> --------------------\n",
      "link:  /BTCTN/status/1734166786747060406\n",
      "text:  Nigerian Web3 Wicrypt Gets Grant From Microsoft to Develop Its DePIN #web3 #arbitrum\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '26', 'repost': '15', 'like': '49', 'view': '11K'}\n",
      "date:  2023-12-11 19:02:35+08:00\n",
      "------------------- tweet<19> --------------------\n",
      "link:  /TheBlock__/status/1734166274375049241\n",
      "text:  ICYMI: M&G Investments leads $30 million raise for GFO-X\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '2', 'repost': '4', 'like': '13', 'view': '5.9K'}\n",
      "date:  2023-12-11 19:00:33+08:00\n",
      "------------------- tweet<20> --------------------\n",
      "link:  /Cointelegraph/status/1734079789965758932\n",
      "text:  A malfunctioning price feed has resulted in over $270,000 of borrowing on @VenusProtocol, but its user funds and other pools are unaffected, according to the DeFi protocol.\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '21', 'repost': '15', 'like': '56', 'view': '22K'}\n",
      "date:  2023-12-11 13:16:54+08:00\n",
      "------------------- tweet<21> --------------------\n",
      "link:  /Cointelegraph/status/1734162359051317699\n",
      "text:  The Venus community will issue a proposal to “immediately inject  liquidity from the treasury to the affected pool totaling around $274K  while funds from the pool are recovered with the support of partners.”\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '3', 'repost': '2', 'like': '14', 'view': '8.7K'}\n",
      "date:  2023-12-11 18:45:00+08:00\n",
      "------------------- tweet<22> --------------------\n",
      "link:  /decryptmedia/status/1734157863268864175\n",
      "text:  $330 Million in Crypto Longs Liquidated as Bitcoin Tumbles Below $43,000\n",
      "       ► https://decrypt.co/209259/330-million-in-crypto-longs-liquidated-as-bitcoin-tumbles-below-43000?utm_source=twitter&utm_medium=social&utm_campaign=auto…\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '2', 'repost': '5', 'like': '18', 'view': '2.8K'}\n",
      "date:  2023-12-11 18:27:08+08:00\n",
      "------------------- tweet<23> --------------------\n",
      "link:  /TheBlock__/status/1734154397838356755\n",
      "text:  Ark Invest’s Coinbase selling spree exceeded $100 million last week\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '4', 'repost': '11', 'like': '41', 'view': '8.9K'}\n",
      "date:  2023-12-11 18:13:22+08:00\n",
      "------------------- tweet<24> --------------------\n",
      "link:  /Cointelegraph/status/1734068338848842182\n",
      "text:  Critics and even employees at @Google have voiced criticism toward the tech firm for adding just a little too much marketing magic into its Gemini tech demo last week.\n",
      "imgs:  None\n",
      "vids:  None\n",
      "stats: {'reply': '19', 'repost': '18', 'like': '49', 'view': '21K'}\n",
      "date:  2023-12-11 12:31:24+08:00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  def launch_browser(login=False, homepage=True):\n",
    "    account = cfg.ACCOUNTS[0]\n",
    "    account['mobile'] = False\n",
    "    mb = Webbroswer(**account)\n",
    "    mb.open_browser()\n",
    "    if login: mb.login()\n",
    "    if homepage: mb.click_following_on_homepage()\n",
    "    return mb\n",
    "\n",
    "  def print_tweets(mb, url):\n",
    "    print('')\n",
    "    print(f' {url} '.center(80, '='))\n",
    "    tn = time.time()\n",
    "    ts = mb.load_tweets(url)\n",
    "    print(f'------ elapsed: {(time.time() - tn):.2f}s' )\n",
    "    mb.print(ts)\n",
    "\n",
    "  def print_tweets_on_following_page(mb: Webbroswer):\n",
    "    print('')\n",
    "    print(f' {cfg.HOME_URL} '.center(80, '='))\n",
    "    tn = time.time()\n",
    "    ts = mb.load_tweets(traceback='1h')\n",
    "    print(f'------ elapsed: {(time.time() - tn):.2f}s' )\n",
    "    mb.print(ts)\n",
    "\n",
    "  def test_url_tweets():\n",
    "    URL1 = 'https://twitter.com/elonmusk'\n",
    "    URL2 = 'https://twitter.com/VitalikButerin'\n",
    "    mb = launch_browser()\n",
    "    time.sleep(5)\n",
    "    print_tweets(mb, URL1)\n",
    "    time.sleep(5)\n",
    "    print_tweets(mb, URL2)\n",
    "    mb.close_browser(clear_browsing=True)\n",
    "\n",
    "  def test_following_tweets():\n",
    "    mb = launch_browser()\n",
    "    time.sleep(2)\n",
    "    print_tweets_on_following_page(mb)\n",
    "\n",
    "  def test_accounts_follow():\n",
    "    accounts = ['elonmusk', 'VitalikButerin']\n",
    "    mb = launch_browser(homepage=False)\n",
    "    time.sleep(4)\n",
    "    mb.follow_accounts(accounts)\n",
    "\n",
    "  def test_accounts_unfollow():\n",
    "    accounts = ['elonmusk', 'VitalikButerin']\n",
    "    mb = launch_browser(homepage=False)\n",
    "    time.sleep(4)\n",
    "    mb.unfollow_accounts(accounts)\n",
    "\n",
    "  test_following_tweets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
